---
title: "Intro to R"
author: "Jess Mankewitz"
date: "6/2/2024"
output: 
  html_document:
    toc: true
    toc_float: true
editor_options: 
  chunk_output_type: inline
---

# Housekeeping

If you have tidyverse installed, you can `knit` the tutorial into an HTML document for better readability by pressing the `knit` button at the top.

## Poll

**POLL QUESTIONS**:

* Do you have R, R Studio, and Tidyverse installed?  [yes / no]
* Have you used tidyverse before?  [yes / no]
* Have you used ggplot before?  [yes / no]


## Misc

The best reference for this material is Hadley Wickham's [R for data scientists](http://r4ds.had.co.nz/). This workshop is adapted from a tutorial Mike Frank and co. put together for an ICIS 2019 tutorial! 

<!-- ----------------------------------------------------------------------- -->
# Goals and Introduction

By the end of this tutorial, you will know:

+ What "tidy data" is and why it's an awesome format
+ How to do some stuff with tidy data
+ How to get your data to be tidy
+ Some new directions for your R analysis moving forward

In order to do that, we'll start by introducing the concepts of **tidy data** and **functions and pipes**.

## Tidy data

All of this is in the `tidyverse` package, which we'll load now. 

```{r setup, include=FALSE}
# install.packages(c("tidyverse", "palmerpenguins", "ggthemes"))
library(tidyverse)

library(palmerpenguins)
library(ggthemes)
```

> “Tidy datasets are all alike, but every messy dataset is messy in its own way.” –– Hadley Wickham

Here's the basic idea: In tidy data, every row is a single **observation** (trial), and every column describes a **variable** with some **value** describing that trial.

And if you know that data are formatted this way, then you can do amazing things, basically because you can take a uniform approach to the dataset. From R4DS:

"There's a general advantage to picking one consistent way of storing data. If you have a consistent data structure, it's easier to learn the tools that work with it because they have an underlying uniformity. There's a specific advantage to placing variables in columns because it allows R's vectorised nature to shine."

Here's an example of a tidy dataset (note: our "datasets" are stored in "dataframes," which in the tidyverse are called "tibbles" - yes, it's a little confusing):

```{r}

```

Each row is one observation (a penguin) and each column is a variable (e.g. bill length) with a value for that observation (e.g. This Adelie penguin's bill length is 39 mm).

## Functions and Pipes

Everything you typically want to do in statistical programming uses **functions**. `mean` is a good example. `mean` takes one **argument**, a numeric vector. Pipes are a way to write strings of functions more easily. They bring the first argument of the function to the beginning. 

What's the mean bill length of the penguins in this dataset? We could pull out the `bill_length_mm` column and take the mean of those values with:

```{r}
```

Equivalently, we can use the pipe symbol `|>` to write:

```{r}

```

The `|>|` means that whatever is before it is passed as the first argument to the function after it.

That's not very useful yet, but when you start **nesting** functions, it gets better.  What if we want the mean height in inches, rounded to two decimal points? 


```{r}
# make a function that divides by 25.4

# first compute mean height in inches the normal, nested way  

# now how do we do this with pipes? 

```

<!-- ----------------------------------------------------------------------- -->
# The briefest of intros to `ggplot`

`ggplot` is the plotting package that's included in `tidyverse`, but it's really a topic in its own right. The easy way to plot something in `ggplot` is to type the kind of plot you want into google and then copy and paste from stack overflow. But if you have to do it by hand, a ggplot has two critical elements:

1. The initial statement, which maps variables in a dataframe to a set of plot aesthetics: this looks like `ggplot(data, aes(x = ..., y = ...))` where `data` is your dataframe and `x` and `y` are followed by the variables you want to map to x and y positions. 

2. One or more "geoms" or geometric mapping elements, like points (`geom_point` with x and y positions) or bars or lines. 

This sounds tricky and it takes a little getting used to, but it's not too bad. 

Let's make a really simple ggplot:

```{r}
# plot penguins mass by bill length

```

Maybe the key awesome thing that works really well in ggplot is something called "faceting" - where you make lots of little plots that are all the same. You do this with `facet_wrap` or `facet_grid`.

```{r}
# try faceting the above plot by sex

```


<!-- ----------------------------------------------------------------------- -->
# Tidy Data Analysis with `dplyr`

Reference: [R4DS Chapter 5](http://r4ds.had.co.nz/transform.html)

Let's take a psychological dataset. Here are the raw data from [Stiller, Goodman, & Frank (2015)](http://langcog.stanford.edu/papers_new/SGF-LLD-2015.pdf). Children met a puppet named "Furble." Furble would show them three pictures, e.g. face, face with glasses, face with hat and glasses and would say "my friend has glasses." They then had to choose which face was Furble's friend. (The prediction was that they'd choose *glasses and not a hat*, indicating that they'd made a correct pragmatic inference). In the control condition, Furble just mumbled. 


These data are tidy: each row describes a single trial, each column describes some aspect of that trial, including their id (`subid`), age (`age`), condition (`condition` - "label" is the experimental condition, "No Label" is the control), item (`item` - which thing Furble was trying to find). 

We are going to manipulate these data using "verbs" from `dplyr`. I'll only teach four verbs, the most common in my workflow (but there are many other useful ones):

+ `filter` - remove rows by some logical condition
+ `mutate` - create new columns 
+ `group_by` - group the data into subsets by some column
+ `summarize` - apply some function over columns in each group  

## Exploring and characterizing the dataset

Notice I'm going to use `read_csv` instead of the usual "base R" `read.csv` - this is the `tidyverse` version and it's faster and has better defaults. (It also returns a "tibble", which is a slightly better version of a data frame. 

```{r}
# read in the data
sgf <- read_csv("data/stiller_scales_data.csv")
```

Inspect the various variables before you start any analysis. Lots of people recommend `summary` but TBH I don't find it useful. 


```{r}

```

I prefer interactive tools like `View` or `DT::datatable` (which I really like, especially in knitted reports).

```{r, eval=FALSE}

```

## Filtering & Mutating

There are lots of reasons you might want to remove *rows* from your dataset, including getting rid of outliers, selecting subpopulations, etc. `filter` is a verb (function) that takes a data frame as its first argument, and then as its second takes the **condition** you want to filter on. 

Note that we're going to be using pipes with functions over data frames here. The way this works is that:

+ `tidyverse` verbs always take the data frame as their first argument, and
+ because pipes pull out the first argument, the data frame just gets passed through successive operations
+ so you can read a pipe chain as "take this data frame and first do this, then do this, then do that."

This is essentially the huge insight of `dplyr`: you can chain verbs into readable and efficient sequences of operations over dataframes, provided 1) the verbs all have the same syntax (which they do) and 2) the data all have the same structure (which they do if they are tidy). 

OK, so filtering:

```{r}
# try to filter SGF so we just get the two year olds

```

** EXERCISE**. Filter out only the "face" trial (item) in the "Label" condition (condition).

(Hint: use the `==` operator to test for equality -- works for strings and numbers.)


```{r}
# face trial, label condition

```

Next up, *adding columns*. You might do this perhaps to compute some kind of derived variable. `mutate` is the verb for these situations - it allows you to add a column. Let's add a discrete age group factor to our dataset. Protip: `cut` is a good function for developmentalists to know because it lets you cut a continuous variable into a set of discrete bins (like we do all the time, for better or for worse). 

```{r}
# add an age group column using the `cut` function

```

## Standard descriptives using `summarise` and `group_by`

We typically describe datasets at the level of subjects, not trials. We need two verbs to get a summary at the level of subjects: `group_by` and `summarise`. Grouping alone doesn't do much.

```{r}
# group by doesn't do much

```

All it does is add a grouping marker. 

What `summarise` does is to *apply a function* to a part of the dataset to create a new summary dataset. So we can apply the function `mean` to the dataset and get the grand mean. 

```{r}
# get the grand mean

```

Note the syntax here: `summarise` takes multiple  `new_column_name = function_to_be_applied_to_data(data_column)` entries in a list. Using this syntax, we can create more elaborate summary datasets also:

```{r}
# more summary 

```

Where these two verbs shine is in combination, though. Because `summarise` applies functions to columns in your *grouped data*, not just to the whole dataset!

So we can group by age or condition or whatever else we want and then carry out the same procedure, and all of a sudden we are doing something extremely useful!

I used to do this:

```{r}
## DO NOT DO THIS!!!
# foo <- initialize_the_thing_being_bound()
# for (i in 1:length(unique(sgf$item))) {
#   for (j in 1:length(unique(sgf$condition))) {
#     this_data <- sgf[sgf$item == unique(sgf$item)[i] & 
#                       sgf$condition == unique(sgf$condition)[n],]
#     do_a_thing(this_data)
#     bind_together_somehow(this_data)
#   }
# }

```


But now I can do this (!):

```{r}
# get means and ns by age group and condition

```

In psychology we often want to first calculate subject-level means per condition so that we can then calculate the variability (standard deviation / standard error).

This is a big gotcha that I see in a lot of work, where someone reports `t(2317) = 400, p < .0000001` but they only had 40 subjects. They forgot to take the subject means and so they did their t-test over every single trial (which makes the p-values very extreme and not right). 

So the thing you really want to do is:

```{r}
# first get subject means
sgf_subject_means <- sgf |> 
  group_by(subid, condition, age_group) |> 
  summarize(mean_correct = mean(correct, na.rm = T))

# then get group means
sgf_means <- sgf_subject_means |> 
  group_by(condition, age_group) |> 
  summarize(se = sd(mean_correct) / sqrt(n()),
           mean_correct = mean(mean_correct),
           n = n())


```

These summary data are typically very useful for plotting.


OK, so now let's go back to the `sgf` data that we had above and make a ggplot of it. 


```{r}
# now make pretty ggplot of sgf by age and condition

```

And we can use `geom_errorbar` to add error bars~


```{r}
# now make pretty ggplot of sgf by age and condition

```

Now what if we care about the by-item differences? What if children are only providing the correct label for faces (but not houses, for example?)

Let's get new means that are also grouped by item in addition to age and condition!

```{r}
# first get subject and item means


# then get group means



```

```{r}
# now make pretty ggplot of sgf by age and condition

```


**Exersizes**. 
Try to work your way through these three exercises, asking questions of your TA. We don't expect everyone to make it through all three in the time we have, don't worry. You're an R ninja if you can do all three 

1. Adapt the code above to find the proportion of correct responses for each item in each condition (rather than for each age group in each condition).

(Hint: change what you `group_by`.)
(Hint 2: "proportion of correct responses" can be computed with `mean`.)

```{r}

```

2. Plot the proportion of correct responses for each item in each condition  `ggplot`. 

```{r}

```


3. Use faceting to make a plot of proportion of correct responses for each item, condition, AND age group. 

(Hint: you will need to go back and `group_by` all of the factors you want in the final plot). 

```{r}

```


<!-- ----------------------------------------------------------------------- -->
# Extras

These extras are fun things to go through at the end of the tutorial, time permitting. Because they require more data and packages, they are set by default not to evaluate if you knit the tutorial. 

## A bigger worked example: Wordbank data

We're going to be using some data on vocabulary growth that we load from the Wordbank database. [Wordbank](http://wordbank.stanford.edu) is a database of children's language learning. 

We're going to look at data from the English Words and Sentences form. These data describe the responses of parents to questions about whether their child says 680 different words. 

`tidyverse` really shines in this context. 

```{r, eval=FALSE}
# to avoid dependency on the wordbankr package, we cache these data. 
# ws <- wordbankr::get_administration_data(language = "English", 
#                                          form = "WS")

ws <- read_csv("data/ws.csv")
```

Take a look at the data that comes out. 

```{r, eval=FALSE}

```


```{r, eval=FALSE}

```

Aside: How can we fix this plot? Suggestions from group? 

```{r, eval=FALSE}

```

OK, let's plot the relationship between sex and productive vocabulary, using `dplyr`.

```{r, eval=FALSE}

```
